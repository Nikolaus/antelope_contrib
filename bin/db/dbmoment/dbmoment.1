.TH DBMOMENT 1
.SH NAME
dbmoment \- moment tensor calculation for given origin
.SH SYNOPSIS
.nf
\fBdbmoment \fP[-vd] [-p \fIpfname\fP] \fIdatabase\fP \fIorid\fP
.fi
.nf
\fBdbmoment \fP[-vd] [-p \fIpfname\fP] \fI-e\fP \fIdatabase\fP \fIevid\fP
.fi

.SH CODE LIMITATIONS
################################

***  READ THIS FIRST  ***

################################

The Moment Tensor calculation tool included in this distribution is
the core component of the more complex Time-Domain Moment Tensor
INVerse Code (TDMT_INVC) toolbox by Douglas Dreger of the Berkeley
Seismological Laboratory that is in use by multiple other networks.
The basics on the methods implemented here are described in Jost and
Herrmann(1989).

The calibration of the tool by the implementation of an appropriate 1-D
velocity model is essential to the successful calculation of a moment
tensor. There are several assumptions described in Dreger's documentation
that limit the types of events that we can analyze to be:
    1) Regional events
    2) No big magnitudes. Mw<7.5

We completed the mapping of the source code and some examples from the
original repo distributed by Dreger. This first version will do all the
data processing in Antelope and removed all SAC dependencies.

.SH USER REQUIREMENTS
The code requires a good 2-D model of the region of interest. Some examples
are provided on the distribution. The full explanation of the values needed
for the model are described on the original PDF distributed by Dreger.
The folder MODELS will be installed in the $ANTELOPE/contrib/ directory.

.SH DATABASE REQUIREMENTS
The code requires some information about the stations, the event(s) that
will be processed, and the actual data files. The list of tables that are
required by the code:
    arrival
    assoc
    event ( if running with the -e flag )
    instrument
    netmag
    origin
    sensor
    site
    wfdisc

The code will later update the 'netmag' and will update (or create) an 'mt'
table with results. There will be a directory with synthetic traces with a
different wfdisc table than the original data.

.SH ACKNOWLEDGEMENT
Moment tensors code extracted from mtpackagev1.1 package developed by
Douglas Dreger of the Berkeley Seismological Laboratory. Green's
functions computed using the FKRPROG software developed by Chandan Saikia
of URS.


.SH DESCRIPTION
The \fBdbmoment\fP application calculates the moment tensor for a given
origin id (\fIorid\fP) for stations with (or without) arrivals associated
to that event. If not configured to use arrivals only then the system can
calculate predicted arrivals to the stations.

The time domain seismic moment tensor inversion software package written
by Dreger (2003) and updated by Minson & Dreger (2008) has been repackaged
for inclusion into the open-source contributed code repository for the Boulder
Real Time Technology (BRTT) Antelope Environmental Monitoring System.

The new code-base utilizes the Python interface to Antelope (Lindquist et al., 2008)
for data access and handling. The new code archives all data products into a
Center for Seismic Studies (CSS) 3.0 schema table \fBmt\fP for easy access and distribution
of solutions. This tool is focused on analyzing local to regional earthquakes. A
calibrated velocity model representative of the south-west continental United States
is included and used for the included examples.

The accompanying Green's functions are either read from the database or generated
dynamically and stored in a dedicated database with a wfdisc. The tool uses a
Frequency-Wavenumber integration program included in Dreger's original distribution.

.SH OPTIONS
.IP -v
Verbose output. Opens final PNG image with results.
.IP -d
Debug output. Opens final PNG image with results.
.IP -e
The provided id in command-line is an EVID instead of an ORID
.IP -x
Debug data processing. Makes an image on every data extraction and waits for user review.
.IP "-f 'BW 0.02 4 0.05 4'"
Use the quoted text as filter instead of option from PF file.
.IP "-p pfname"
Override the default parameter-file \fBdbmoment.pf\fP
.IP "-m SOCAL_MODEL"
Use a different velocity model instead of the PF configured value. \fBSOCAL_MODEL.pf\fP
.IP "-c value"
Set a new minimum correlation value threshold instead of using the one in the PF.
.IP "-s 'STA1|STA2|STA3'"
String to subset the list of stations to use.
.IP "-r 'STA1|STA2|STA3'"
String to reject stations.
.IP "-z 'STA1:3,STA2:3,STA3:5'"
Forced ZCOR values of some stations. This is a shift in time.


.SH PARAMETER FILE
Some important values from the parameter file:

.IP \fIantelope\fP
Global variable to make full path later in the PF file.

.IP \fImodel_file\fP
Model for the production of synthetic traces. Just the name. Look
for the file in the "model_path" list. Overwrite with -m flag if needed.

.IP \fImodel_path\fP
All directories to search for the velocity models. Stop on first match.

.IP \fItmp_folder\fP
All temporary files will be written to this folder. Default "./dbmoment/".

.IP \fIimage_folder\fP
All results will produce an image that will be archived in this folder.

.IP \fIclean_tmp\fP
If True then we clean the temporary folder before exiting the code. False will
keep all temporary files in the folder. Good for troubleshooting problems with
the tool.

.IP \fIchan_to_use\fP
Channels which are used in the inversion.

.IP \fIdepth_min/depth_max\fP
Only work with event depths within this range. Verify with values in MODEL!

.IP \fIsta_max\fP
Only calculate the inversion for no more than this amount.

.IP \fIsta_min\fP
Avoid running the inversion if we don't get at least this amount of stations.

.IP \fIfind_executables\fP
Look for these names on the PATH and keep the full path to them in memory. Replace
the path on some scripts that we create on the tmp_folder.

.IP \fIrecursive_analysis\fP
After the first inversion try to remove traces with bad correlations. If we have
unused stations then add the new sites and re-run inversion.

.IP \fImin_variance\fP
Avoid stations with variance reduction lower than this.

.IP \fImin_quality\fP
Each inversion will return a QUALITY value from 0 to 4. Set this as a filter to
avoid injecting bad quality inversions to the database.

.IP \fIfilter_individual_mw\fP
Each station is inverted individually and a max correlation and individual
magnitude is produced. At this point we can avoid adding stations that are
not matching the expected magnitude already present in the database.

.SH EXAMPLE

Dreger's original code contains an example dataset for users to test the code. The
EXAMPLE_1 from the original distribution was migrated to an Antelope database
consisting of a wfdisc table, an origin and event tables and associated dbmaster
tables needed. The records on the original database are already rotated
to ZRT, calibrated, filtered and instrument response corrected.

A new dbbuild batch file was created to put some generic metadata for stations.
Generic stations names [STA1, STA2, STA3] are used. There is an EVID=1 in the database
with correct arrival times, azimuths, and distances to the stations.
This event is located at 100 km from each site and azimuths of [10,40,50].

There is a second database in EVENT_2 example folder with raw data files and
metadata downloaded from IRIS for those sites.

You can run both examples by simply using the command:

     \fBdbmoment_run_example\fP

If you want to run each individual example by hand then you can:

    % cd $(ANTELOPE)/contrib/example/dbmoment/
    % \fBrm\fP -rf synthetics_db
    % \fBrm\fP -rf .dbmoment
    % \fB dbmoment\fP -v EXAMPLE_1/example_1 1
    % \fB dbmoment\fP -v EXAMPLE_2/example_2 1

All temporary working files will go into the local .dbmoment/ directory, the images with
results will go into dbmoment_images/ and synthetics will be saved in synthetics_db/
folder.


.SH VELOCITY MODELS
We are collecting all velocity models into a dedicated folder inside the repo
and copying this folder to the contributed folder structure in the main code
distribution folder. The files have a ParameterFile structure and are named in the same
way any other parameter file is but are not included in the general
repository for PF files in Antelope. The implications are that your dbmoment.pf
file will need a full path to the velocity model file because it will not be in
the PFPATH environmental configuration.
The best option will be for you create your own velocity models and
to keep them in a local folder and list this on the dbmoment.pf configuration.
We also appreciate greatly if you can upload your local velocity model to the
contributed code repository and make them available to anyone analyzing
events in similar locations.

.SH CODE STEPS
 First step for dbmoment is for the code to open the event database and
 extract all event information from the tables and identify the stations needed.
 This will look into any other reported magnitude and associated arrivals to the event.
 Having a magnitude calculated for the event is a fundamental step that will change
 the configuration selected for the analysis of the event.

 All stations are evaluated to see if they fit the requirements for distance.
 In case we are using existing arrivals only for the inversion then no
 theoretical arrivals are calculated for the selected channels.

 The code will then extract the traces for each of the selected stations and
 will fetch synthetics for each depth-distance combination between the stations and
 the origin of the event. If the synthetics are not present in our database then
 dbmoment will create the traces dynamically. In case of new synthetics are produced
 then we save them in a dedicated wfdisc table for synthetics. The name of the model
 used is important in this database. If you change the model then the process will
 generate new synthetic traces. If you change values within the configuration of your
 model then you should delete the previous archive of synthetics and allow the
 process to generate new traces.

 All valid stations will get a first round of individual inversions for this event.
 This will give us the maximum correlation and time shift that we can get from that
 site with respect to the synthetics.

 Then we subset for the stations with correlations above our limit and we order
 them starting with the best variance reduction. We select the first group for inversion
 to the max number set in the parameter file. After that first inversion the code
 will exit or continue to a secondary review if the recursive flag is set to True
 in the parameter file.

 If we are running recursively then we select the worst performer of the group and
 we evaluate the variance reduction. If we find the variance reduction under
 our threshold then we remove the site and we try to add a new station.
 We send the new group to the next inversion and we evaluate the results one more time.

 This recursive method continues until:
    1) All sites variance reduction are above our threshold limit
    2) We run out of stations.

 Running on (-v) or (-d) mode will produce a plot at the end script that will
 compare the original traces with the theoretical calculations for each station
 based on our synthetics and the values of the tensor returned by the tool.

 At the end of every run the system will update the “mt” table and the “netmag”
 tables with the results. If a previous entry for the same ID and AUTH is found on the
 tables then we remove the old entry before adding a new row with the new results.

.SH SYNTHETIC TRACES
Synthetics are archived on a wfdisc table using a schema based on depth and distance to the
event. We use a model of lazy evaluation which delays the creation of a synthetic trace
until its value is needed.

The value for the station name is our DEPTH to the event. The value for the channel
is our event to station DISTANCE and the seismic element is specified in the
LOC code of the channel name.
i.e.
    depth: 8
    distance: 10
    element: TDS
    => 8_10_TDS ( format: sta_chan_loc )

The format allows us to clearly see all traces related to the same depth on the
dbpick window organized by distance. It inverts the originally proposed schema
but the benefits justify the changes. If you ran versions of the code earlier
than 1/2016 then you might need to remove the synthetic databases and allow the
software to produce new versions of it.

.SH SEE ALSO
dbmoment_run_example(1)
antelope_python(3y)

.SH AUTHOR
Juan Reyes (UCSD)

.SH COLLABORATORS
.nf
Matt Koes (PGC, Canada/UCSD)
Rob Newman (UCSD)
Gert-Jan van den Hazel (Orfeus Data Center/UCSD)
.fi
